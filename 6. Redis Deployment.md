####
---
Redis Deployment (as Celery broker):
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  selector:
    app: redis
  ports:
  - protocol: TCP
    port: 6379
    targetPort: 6379
```
---
Celery Worker Deployment:
- This connects to the same image used by the AI API (FastAPI) because it will share the same codebase
```celery-worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-worker
  labels:
    app: celery
spec:
  replicas: 2
  selector:
    matchLabels:
      app: celery
  template:
    metadata:
      labels:
        app: celery
    spec:
      containers:
      - name: celery-worker
        image: image-registry.openshift-image-registry.svc:5000/<namespace>/fastapi-ai-api:latest
        command: ["celery", "-A", "celery_worker.celery_app", "worker", "--loglevel=info"]
        env:
        - name: REDIS_URL
          value: "redis://redis:6379/0"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```
---
Update AI API Deployment to include Celery environment:
- Add environment variables so that FastAPI can send tasks to Celery -> Update your fastapi-ai-api Deployment with:
```yml
        env:
        - name: REDIS_URL
          value: "redis://redis:6379/0"
```
---
Celery Worker Python Code (celery_worker.py):
- Place this in the same project directory as your FastAPI app:
```python
from celery import Celery
import json
import time

# Celery configuration
app = Celery('tasks', broker='redis://redis:6379/0', backend='redis://redis:6379/0')

@app.task
def process_aap_logs(log_text: str):
    """Simulate AI parsing + KB lookup for logs"""
    # Simulated heavy parsing logic
    time.sleep(2)

    # Example: check for keywords
    if "Failed to connect" in log_text:
        return {"issue": "SSH Connection Failure", "solution": "Verify credentials and network connectivity."}
    elif "No space left" in log_text:
        return {"issue": "Disk Full", "solution": "Clean up /tmp or add more disk space."}
    else:
        return {"issue": "Unknown", "solution": "Please review manually."}
```
---
Workflow now:
- FastAPI receives logs → sends task to Celery via Redis → Celery processes → returns job ID → FastAPI polls result or streams back later.
---
Do you want me to now:
✔ Write FastAPI routes that send logs to Celery and return task ID
✔ Add a /result/{task_id} endpoint to fetch Celery result
✔ Update Streamlit UI to call these APIs asynchronously?
